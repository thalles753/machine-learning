{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (45000, 32, 32, 3), (45000,))\n",
      "('Validation set', (5000, 32, 32, 3), (5000,))\n",
      "('Test set', (10000, 32, 32, 3), (10000,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'dataset.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (45000, 32, 32, 3), (45000, 10))\n",
      "('Validation set', (5000, 32, 32, 3), (5000, 10))\n",
      "('Test set', (10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "num_labels = 10\n",
    "depth = 3\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape (45000, 10)\n",
      "First label vector [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Second label vector [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print 'Training labels shape', train_labels.shape\n",
    "print 'First label vector', train_labels[0]\n",
    "print 'Second label vector', train_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a Tensor Weight with a predefined shape\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# create a Tensor Biases with a predifined shape\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# perform a convolution using strides of 1 and SAME padding\n",
    "# reducing the kernel stride to 1 will make the convolutions output images with the same\n",
    "# size as the original input\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# perform a max pooling operation using stride of 2\n",
    "# apply max pooling (Windows size 2x2) and Stride of 2 to reduce\n",
    "# the image dimension by half\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, \n",
    "                        ksize=[1, 2, 2, 1], # The size of the window for each dimension of the input tensor.\n",
    "                        strides=[1, 2, 2, 1], # The stride of the sliding window for each dimension of the input \n",
    "                        padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SIZE=32\n",
    "NUM_LABELS=10\n",
    "# We'll bundle groups of examples during training for efficiency.\n",
    "# This defines the size of the batch.\n",
    "BATCH_SIZE = 60\n",
    "# We have only one channel in our grayscale images.\n",
    "NUM_CHANNELS = 3\n",
    "# The random seed that defines initialization.\n",
    "SEED = 42\n",
    "\n",
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step, which we'll write once we define the graph structure.\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, NUM_LABELS))\n",
    "\n",
    "# For the validation and test data, we'll just hold the entire dataset in\n",
    "# one constant node.\n",
    "validation_data_node = tf.constant(valid_dataset)\n",
    "test_data_node = tf.constant(test_dataset)\n",
    "\n",
    "# The variables below hold all the trainable weights. For each, the\n",
    "# parameter defines how the variables will be initialized.\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE / 4 * IMAGE_SIZE / 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, NUM_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "  \"\"\"The Model definition.\"\"\"\n",
    "  # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "  # the same size as the input). Note that {strides} is a 4D array whose\n",
    "  # shape matches the data layout: [image index, y, x, depth].\n",
    "  conv = tf.nn.conv2d(data,\n",
    "                      conv1_weights,\n",
    "                      strides=[1, 1, 1, 1],\n",
    "                      padding='SAME')\n",
    "\n",
    "  # Bias and rectified linear non-linearity.\n",
    "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "  \n",
    "  # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "  # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "  pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "  conv = tf.nn.conv2d(pool,\n",
    "                      conv2_weights,\n",
    "                      strides=[1, 1, 1, 1],\n",
    "                      padding='SAME')\n",
    "  relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "  pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "  \n",
    "  # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "  # fully connected layers.\n",
    "  pool_shape = pool.get_shape().as_list()\n",
    "  reshape = tf.reshape(\n",
    "      pool,\n",
    "      [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "  # Fully connected layer. Note that the '+' operation automatically\n",
    "  # broadcasts the biases.\n",
    "  hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "  \n",
    "  # Add a 50% dropout during training only. Dropout also scales\n",
    "  # activations such that no rescaling is needed at evaluation time.\n",
    "  if train:\n",
    "    hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "  return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  logits, train_labels_node))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  4 * BATCH_SIZE,      # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a new interactive session that we'll use in\n",
    "# subsequent code cells.\n",
    "s = tf.InteractiveSession()\n",
    "\n",
    "# Use our newly created session as the default for \n",
    "# subsequent operations.\n",
    "s.as_default()\n",
    "\n",
    "# Initialize all the variables we defined above.\n",
    "tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 60\n",
    "\n",
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_dataset[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph is should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = s.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.00776582e-06   5.22931397e-04   6.66658347e-03   1.11271262e-01\n",
      "   3.06268851e-03   2.12299358e-02   1.08794849e-02   5.82005130e-04\n",
      "   1.35196103e-02   8.32258403e-01]\n"
     ]
    }
   ],
   "source": [
    "print predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 9\n",
      "(60, 10)\n",
      "All predictions [9 2 0 7 4 9 8 0 9 9 9 7 9 8 7 2 7 7 3 9 4 3 7 9 1 9 7 0 9 4 1 8 7 0 7 2 4\n",
      " 3 7 0 7 7 7 5 7 2 0 7 7 9 2 0 8 8 0 7 9 9 9 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The highest probability in the first entry.\n",
    "print 'First prediction', np.argmax(predictions[0])\n",
    "\n",
    "# But, predictions is actually a list of BATCH_SIZE probability vectors.\n",
    "print predictions.shape\n",
    "\n",
    "# So, we'll take the highest probability for each vector.\n",
    "print 'All predictions', np.argmax(predictions, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 1 6 2 8 6 1 2 6 7 7 7 3 1 5 1 4 2 1 7 6 3 3 9 1 5 3 3 3 8 9 0 4 5 5 5 6\n",
      " 5 5 0 9 1 0 9 6 3 7 1 3 7 9 0 6 1 3 5 4 9 7 8]\n"
     ]
    }
   ],
   "source": [
    "print 'Batch labels', np.argmax(batch_labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.116666666667\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPcAAAD7CAYAAAC2TgIoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e9b61bdd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "correct = np.sum(np.argmax(predictions, 1) == np.argmax(batch_labels, 1))\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print float(correct) / float(total)\n",
    "\n",
    "confusions = np.zeros([10, 10], np.float32)\n",
    "bundled = zip(np.argmax(predictions, 1), np.argmax(batch_labels, 1))\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(NUM_LABELS))\n",
    "plt.yticks(np.arange(NUM_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');\n",
    "print \"Done.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "  \"\"\"Return the error rate and confusions.\"\"\"\n",
    "  correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "  total = predictions.shape[0]\n",
    "\n",
    "  error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "  confusions = np.zeros([10, 10], np.float32)\n",
    "  bundled = zip(np.argmax(predictions, 1), np.argmax(labels, 1))\n",
    "  for predicted, actual in bundled:\n",
    "    confusions[predicted, actual] += 1\n",
    "    \n",
    "  return error, confusions\n",
    "\n",
    "print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 750\n",
      "Mini-batch loss: 9.61714 Error: 85.00000 Learning rate: 0.01000\n",
      "Step 100 of 750"
     ]
    }
   ],
   "source": [
    "# Train over the first 1/4th of our training set.\n",
    "steps = int(train_labels.shape[0] / BATCH_SIZE)\n",
    "for step in xrange(steps):\n",
    "        \n",
    "  # Compute the offset of the current minibatch in the data.\n",
    "  # Note that we could use better randomization across epochs.\n",
    "  offset = (step * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n",
    "  batch_data = train_dataset[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "  batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "  # This dictionary maps the batch data (as a numpy array) to the\n",
    "  # node in the graph is should be fed to.\n",
    "  feed_dict = {train_data_node: batch_data,\n",
    "               train_labels_node: batch_labels}\n",
    "  # Run the graph and fetch some of the nodes.\n",
    "  _, l, lr, predictions = s.run(\n",
    "    [optimizer, loss, learning_rate, train_prediction],\n",
    "    feed_dict=feed_dict)\n",
    "\n",
    "  # Print out the loss periodically.\n",
    "  if step % 100 == 0:\n",
    "    error, _ = error_rate(predictions, batch_labels)\n",
    "    print 'Step %d of %d' % (step, steps)\n",
    "    print 'Mini-batch loss: %.5f Error: %.5f Learning rate: %.5f' % (l, error, lr)\n",
    "    #print 'Validation error: %.1f%%' % error_rate(\n",
    "    #    validation_prediction.eval(), valid_labels)[0]\n",
    "    \n",
    "print 'Validation error: %.1f%%' % error_rate(\n",
    "      test_prediction.eval(), test_labels)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "%matplotlib inline\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip argument #3 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e1ece5a995ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mconfusions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mbundled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbundled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0mconfusions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: zip argument #3 must support iteration"
     ]
    }
   ],
   "source": [
    "batch_labels = np.array([1,4,3,2,4,5,6,4,3,4,5,7])\n",
    "predictions = np.array([3,2,4,3,5,6,5,6,7,8,3,4])\n",
    "\n",
    "correct = np.sum(predictions == batch_labels)\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print float(correct) / float(total)\n",
    "\n",
    "confusions = np.zeros([10, 10], np.float32)\n",
    "bundled = zip(predictions, batch_labels, 1)\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(NUM_LABELS))\n",
    "plt.yticks(np.arange(NUM_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
