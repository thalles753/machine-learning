{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thalles/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pdb\n",
    "from time import sleep\n",
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from scipy import misc\n",
    "from skimage import color\n",
    "from collections import deque\n",
    "import random\n",
    "%matplotlib inline\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PIXEL_DEPTH=255.\n",
    "IMAGE_SIZE=84\n",
    "def process_input(img):\n",
    "    out = img[:171, :] # get only the playing area of the image\n",
    "    out = misc.imresize(out, (IMAGE_SIZE,IMAGE_SIZE))\n",
    "    r, g, b = out[:,:,0], out[:,:,1], out[:,:,2]\n",
    "    out = r * (299./1000.) + r * (587./1000.) + b * (114./1000.)\n",
    "    out = (out - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAME_NAME=\"MsPacman-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-05 22:33:55,266] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image origial shape: (210, 160, 3)\n",
      "New image shape: (84, 84)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x119fde7d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1175d29d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(GAME_NAME)\n",
    "env.reset()\n",
    "\n",
    "img = env.render('rgb_array')\n",
    "print \"Image origial shape:\", img.shape\n",
    "# plt.imshow(img, cmap='gray', interpolation='nearest')\n",
    "\n",
    "out = process_input(img)\n",
    "print \"New image shape:\", out.shape\n",
    "plt.imshow(out.squeeze(), cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_FRAMES_PER_STATE=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 999\n",
    "\n",
    "def weight_variable(name, shape):\n",
    "  return tf.get_variable(name=name, shape=shape, initializer=tf.contrib.layers.xavier_initializer(seed=SEED))\n",
    "\n",
    "def weight_conv_variable(name, shape):\n",
    "  return tf.get_variable(name=name, shape=shape, \n",
    "                         initializer=tf.contrib.layers.xavier_initializer_conv2d(seed=SEED))\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.zeros_initializer(shape=shape, dtype=tf.float32)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x, strides=[1, 2, 2, 1]):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=strides, padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CONV1_DEPTH=16\n",
    "W_conv1 = weight_conv_variable(\"conv1\", [8, 8, NUM_FRAMES_PER_STATE, CONV1_DEPTH])\n",
    "b_conv1 = bias_variable([CONV1_DEPTH])\n",
    "\n",
    "CONV2_DEPTH=32\n",
    "W_conv2 = weight_conv_variable(\"conv2\", [4, 4, CONV1_DEPTH, CONV2_DEPTH])\n",
    "b_conv2 = bias_variable([CONV2_DEPTH])\n",
    "\n",
    "FC1_SIZE = 255\n",
    "W_fc1 = weight_variable(\"fc1\", [11 * 11 * CONV2_DEPTH, FC1_SIZE])\n",
    "b_fc1 = bias_variable([FC1_SIZE])\n",
    "\n",
    "out_layer = weight_variable(\"out1\", [FC1_SIZE, NUM_ACTIONS])\n",
    "bias_layer = bias_variable([NUM_ACTIONS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(inputs):\n",
    "    # convolves 16 8×8 filters with stride 4\n",
    "    h_conv1 = tf.nn.relu(conv2d(inputs, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool(h_conv1, strides=[1, 4, 4, 1])\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool(h_conv2, strides=[1, 2, 2, 1])\n",
    "\n",
    "    shape = h_pool2.get_shape().as_list()\n",
    "    h_pool_flat = tf.reshape(h_pool2, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "    # First fully connected layer\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool_flat, W_fc1) + b_fc1)\n",
    "    q_actions = tf.matmul(h_fc1, out_layer) + bias_layer\n",
    "\n",
    "    return q_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_state(state):\n",
    "    assert state.shape == (IMAGE_SIZE, IMAGE_SIZE, 4)\n",
    "    f, axarr = plt.subplots(1, 4, figsize=(14,14))\n",
    "    for i in range(4):\n",
    "        img = state[:,:,i]\n",
    "        axarr[i].imshow(img, cmap=plt.cm.Greys);\n",
    "        axarr[i].set_title('Image: ' + str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LearningAgent:\n",
    "    def __init__(self, env, is_training=True):\n",
    "        # game variables and constants\n",
    "        self._env = env\n",
    "        self.state = None\n",
    "        self.ACTION_NAMES = self._env.get_action_meanings()\n",
    "        self.NUM_ACTIONS = env.action_space.n\n",
    "        self.action = 0 # default first action\n",
    "        self.total_reward = 0\n",
    "        self.rewards_per_game = []\n",
    "        self.is_training = is_training\n",
    "        \n",
    "        # AI variables and constants\n",
    "        self.REPLAY_MEN = 10000 # The paper remember the 1.000.000 most recent frames\n",
    "        self.exp_replay_list = deque(maxlen=self.REPLAY_MEN)\n",
    "        self.SKIP_FRAME_RATE = 4\n",
    "        self.NUM_FRAMES_PER_STATE = 4\n",
    "        self.OBSERVATION_STEPS = 500 # Papers value: 50000\n",
    "        \n",
    "        # Q LEARNING variables and constants\n",
    "        self.INITIAL_EXPLORATION_PROB = 1.0 # Exploration probability\n",
    "        self.FINAL_EXPLORATION_PROB = 0.1\n",
    "        self.learning_rate = 0.00025 # lr from the paper 0.00025\n",
    "        self.discount_factor = 0.99 # dicount factor from the paper 0.99\n",
    "        self.MINI_BATCH_SIZE = 35 # minibatch size from the paper 35\n",
    "        \n",
    "        # Tensorflow variables\n",
    "        self._session = tf.Session() \n",
    "        self._input = tf.placeholder(tf.float32, shape=(None,) + (IMAGE_SIZE,IMAGE_SIZE,self.NUM_FRAMES_PER_STATE), name=\"input_images\")\n",
    "        self._target = tf.placeholder(tf.float32, [None], name=\"input_targets\")\n",
    "        self._action = tf.placeholder(tf.float32, [None, self.NUM_ACTIONS], name=\"input_actions\")\n",
    "        \n",
    "        # Create Deep model\n",
    "        self.model_output = model(self._input)\n",
    "        \n",
    "        # train variables\n",
    "        readout_action = tf.reduce_sum(tf.mul(self.model_output, self._action), reduction_indices=1)\n",
    "        self.loss = tf.reduce_mean(tf.square(self._target - readout_action))\n",
    "        self.train_operation = tf.train.RMSPropOptimizer(self.learning_rate, momentum=0.95, epsilon=0.01).minimize(self.loss)\n",
    "        \n",
    "        self._session.run(tf.initialize_all_variables())\n",
    "    \n",
    "    def to_one_hot_vec(self, actions):\n",
    "        one_hot = np.zeros((self.MINI_BATCH_SIZE, self.NUM_ACTIONS))\n",
    "        one_hot[np.arange(self.MINI_BATCH_SIZE), actions] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def train(self):\n",
    "        # Sample random minibatch of transitions (s_i, a_i, r_i, s_i+1)\n",
    "        # from the experience replay list\n",
    "        mini_batch = random.sample(self.exp_replay_list, self.MINI_BATCH_SIZE)\n",
    "        previous_states = [d[0] for d in mini_batch]\n",
    "        actions = [d[1] for d in mini_batch]\n",
    "        rewards = [d[2] for d in mini_batch]\n",
    "        current_states = [d[3] for d in mini_batch]\n",
    "        \n",
    "        # clip rewards between -1 and 1\n",
    "        rewards = np.clip(rewards, a_min=0, a_max=1)\n",
    "        \n",
    "        agents_expected_reward = []\n",
    "        # this gives us the agents expected reward for each action we might take\n",
    "        agents_reward_per_action = self._session.run(self.model_output, feed_dict={self._input: current_states})\n",
    "        \n",
    "        for i in range(len(mini_batch)):\n",
    "            if mini_batch[i][4]:\n",
    "                # this was a terminal frame so there is no future reward...\n",
    "                agents_expected_reward.append(rewards[i])\n",
    "            else:\n",
    "                # compute r_j + γ max Q(s_j, a; θ), \n",
    "                #    where:\n",
    "                #      s_j is the current state\n",
    "                #      θ are the CNNs learned weights\n",
    "                agents_expected_reward.append(\n",
    "                    rewards[i] + self.discount_factor * np.max(agents_reward_per_action[i]))\n",
    "        \n",
    "        print agents_expected_reward.shape\n",
    "        print agents_expected_reward\n",
    "        \n",
    "        # Perform a gradient descent step on (y_j − Q(φ_j, a_j; θ))^2\n",
    "        _, loss = self._session.run([self.train_operation, self.loss], feed_dict={\n",
    "            self._input: previous_states,\n",
    "            self._action: self.to_one_hot_vec(actions),\n",
    "            self._target: agents_expected_reward})\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def reset_env(self):\n",
    "        self.rewards_per_game.append(self.total_reward)\n",
    "        self.total_reward = 0\n",
    "        return self._env.reset()\n",
    "\n",
    "    def get_random_action(self):\n",
    "        return self._env.action_space.sample()\n",
    "\n",
    "    def get_action_name(self, action):\n",
    "        return self.ACTION_NAMES[action]\n",
    "    \n",
    "    # for the first step, the state is the same state repeated [STATE_FRAMES] times\n",
    "    def game_initial_setup(self):\n",
    "        observations = process_input(self.reset_env())\n",
    "        self.state = np.stack(tuple(observations for _ in range(self.NUM_FRAMES_PER_STATE)), axis=2)\n",
    "        print \"First state:\", self.state.shape\n",
    "        \n",
    "    \n",
    "    def get_next_action(self, step):\n",
    "        # With probability p select a random action (a) otherwise select a = maxaQ∗(φ(st), a; θ)\n",
    "        p = self.get_exploration_probability(step)\n",
    "        \n",
    "        if step % 70 == 0:\n",
    "            print \"Epsilon:\", p\n",
    "\n",
    "        if random.random() < p:\n",
    "            out = self.get_random_action()\n",
    "        else:\n",
    "            current_state = np.expand_dims(self.state, axis=0)\n",
    "            q_action = self._session.run(self.model_output, feed_dict={self._input: current_state})\n",
    "            out = np.argmax(q_action)\n",
    "        return out\n",
    "    \n",
    "    # perform a linear decay operation\n",
    "    def get_exploration_probability(self, global_step):\n",
    "        return (self.FINAL_EXPLORATION_PROB + \n",
    "                max(0, (self.INITIAL_EXPLORATION_PROB - self.FINAL_EXPLORATION_PROB) * \n",
    "                    (self.REPLAY_MEN - max(0, global_step - self.OBSERVATION_STEPS)) / self.REPLAY_MEN))\n",
    "    \n",
    "    def update(self, step):\n",
    "        new_observation, reward, done, info = None, None, None, None\n",
    "        self._env.render()\n",
    "        \n",
    "        if step % self.SKIP_FRAME_RATE == 0:    \n",
    "            self.action = self.get_next_action(step)\n",
    "\n",
    "        # Execute action a_t in emulator and observe reward r_t and image x_t+1\n",
    "        new_observation, reward, done, info = self._env.step(self.action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        # Set s_t+1 = s_t, a_t, x_t+1 and preprocess s_t+1 = φ(st+1)\n",
    "        new_observation = process_input(new_observation)\n",
    "        new_observation = np.expand_dims(new_observation, axis=2)\n",
    "\n",
    "        # store transition (φ_t, a_t, r_t, φ_t+1) in D (experience decay collection)\n",
    "        new_state = np.append(self.state[:, :, 1:], new_observation, axis=2)\n",
    "        self.exp_replay_list.append([self.state, self.action, reward, new_state, done])\n",
    "\n",
    "        # only train if done observing\n",
    "        if len(self.exp_replay_list) > self.OBSERVATION_STEPS and self.is_training:\n",
    "            # Perform a gradient descent step on (y_j −Q(φ_j, a_j; θ))^2 \n",
    "            loss = self.train()\n",
    "\n",
    "            if step % 70 == 0:\n",
    "                print \"Step: \", step, \"Action:\", self.get_action_name(self.action), \"Loss:\", loss\n",
    "\n",
    "            #print_state(self.exp_replay_list[self.count][0])\n",
    "            #print\"----------------------\"\n",
    "            #print_state(self.exp_replay_list[self.count][3])  \n",
    "            #self.count += 1\n",
    "\n",
    "        self.state = new_state\n",
    "        \n",
    "        if done:\n",
    "            self.reset_env()\n",
    "            return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First state: (84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "agent = LearningAgent(env)\n",
    "agent.game_initial_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "#saver = tf.train.Saver()\n",
    "#save_path = saver.save(agent._session, \"./model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 1.0\n",
      "Epsilon: 1.0\n",
      "Epsilon: 1.0\n",
      "Epsilon: 1.0\n",
      "Episode finished after 434 timesteps\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ffc9729753dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOTAL_TRAIN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# sleep(DELAY)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-0be6d59d046b>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_replay_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBSERVATION_STEPS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m# Perform a gradient descent step on (y_j −Q(φ_j, a_j; θ))^2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m70\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-0be6d59d046b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m                     rewards[i] + self.discount_factor * np.max(agents_reward_per_action[i]))\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0magents_expected_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0magents_expected_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "DELAY = 0.05\n",
    "TOTAL_TRAIN_STEPS = 100000 # article trained with 10.000.000 million steps\n",
    "\n",
    "for step in range(TOTAL_TRAIN_STEPS):\n",
    "    \n",
    "    done = agent.update(step)\n",
    "\n",
    "    # sleep(DELAY)\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.1\n",
      "Episode finished after 66 timesteps\n",
      "Epsilon: 0.1\n",
      "Epsilon: 0.1\n",
      "Epsilon: 0.1\n",
      "Episode finished after 534 timesteps\n",
      "Epsilon: 0.1\n",
      "Epsilon: 0.1\n",
      "Epsilon: 0.1\n",
      "Episode finished after 972 timesteps\n",
      "Epsilon: 0.1\n",
      "Epsilon: 0.1\n",
      "Epsilon: 0.1\n",
      "Epsilon: 0.1\n",
      "Episode finished after 1407 timesteps\n",
      "Epsilon: 0.1\n",
      "Epsilon: 0.1\n",
      "Epsilon: 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-3141e5c6907f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOTAL_TEST_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# sleep(DELAY)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-ff99c11e70f2>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Execute action a_t in emulator and observe reward r_t and image x_t+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mnew_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thalles/Desktop/Code/reinforcement-learning/gym/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thalles/Desktop/Code/reinforcement-learning/gym/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/thalles/anaconda/lib/python2.7/site-packages/atari_py/ale_python_interface.pyc\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DELAY = 0.05\n",
    "TOTAL_TEST_STEPS = 100000 # article trained with 10.000.000 million steps\n",
    "agent.is_training = False\n",
    "# agent.INITIAL_EXPLORATION_PROB = 0.05\n",
    "for step in range(TOTAL_TEST_STEPS):\n",
    "    \n",
    "    done = agent.update(step)\n",
    "\n",
    "    # sleep(DELAY)\n",
    "\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = agent.exp_replay_list[0][3]\n",
    "print img.shape\n",
    "#print agent.state[:, :, 1].shape\n",
    "plt.imshow(img[:,:,3], cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
