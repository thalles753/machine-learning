Justify why you picked these set of states, and how they model the agent and its environment.

We can think about states as things that describe the world, in other words, it is a set of possibilities that represents the possible states that an agent might be in. Thus, since the world of the smartcabe is basically a grid-like (matrix) system, one possible way of representing every place an agente can be in, is by representing each of the possible intersections of this grid as a unique state. However, some of these intersections have different semantics from the others which is the case of the start intersection (the intersection in which the smartcab starts in the game), that can be represented as the start state of the agent, and the destination intersection which can be described as the destination state. However, although this strategy seems reasonable to choose valid states for this problem setup, I believe there is a better approach to take on this issue. When we think about rewards, we know that rewards are scalars (let's say points) that an agent gets when being in a particular state. Based on this definition, it makes sense then, to map the possible states of an agent in a world based on the game's rewards setup. The problem defines that an agent gets a reward for each successfully completed trip, thus, we can define the destination state as one of the possible states of the game. Also, the agent will get a small positive reward for each corrected executed move (valid move state), a negative reward for each incorrect executed move (incorrect move), and finally a larger negative reward for each violation of the traffic rules (traffic violated state). In my opinion, this last appoach better models the agent and its enviroment compared to the first because it is a smaller set that describes exactly the same information the previous could without repetition. 

rou